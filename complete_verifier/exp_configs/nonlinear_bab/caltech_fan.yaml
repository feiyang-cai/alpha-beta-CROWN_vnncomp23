model:
  name: Customized("../../models/control/neural_lyapunov_control.py", "CaltechFan")
  path: models/control/caltech_fan.pth
data:
  dataset: Customized("../../models/control/neural_lyapunov_control.py", "caltech_fan_data")
  num_outputs: 1
specification:
  robustness_type: all-positive
  type: lp
  epsilon: 2.0
  norm: 2
attack:  # Currently attack is only implemented for Linf norm.
  pgd_order: before
  pgd_steps: 100  # Increase for a stronger attack. A PGD attack will be used before verification to filter on non-robust data examples.
  pgd_restarts: 30  # Increase for a stronger attack.
solver:
  batch_size: 512
  alpha-crown:
    lr_decay: 0.999
    iteration: 200
  beta-crown:
    lr_alpha: 0.5
    lr_beta: 0.1
    iteration: 50
# solver:
#   batch_size: 2048  # Number of subdomains to compute in parallel in bound solver. Decrease if you run out of memory.
#   alpha-crown:
#     iteration: 100   # Number of iterations for alpha-CROWN optimization. Alpha-CROWN is used to compute all intermediate layer bounds before branch and bound starts.
#     lr_alpha: 0.1    # Learning rate for alpha in alpha-CROWN. The default (0.1) is typically ok.
#   beta-crown:
#     lr_alpha: 0.01  # Learning rate for optimizing the alpha parameters, the default (0.01) is typically ok, but you can try to tune this parameter to get better lower bound.
#     lr_beta: 0.05  # Learning rate for optimizing the beta parameters, the default (0.05) is typically ok, but you can try to tune this parameter to get better lower bound.
#     iteration: 20  # Number of iterations for beta-CROWN optimization. 20 is often sufficient, 50 or 100 can also be used.
bab:
  pruning_in_iteration: false   # bug
  branching:
    method: nonlinear
    candidates: 1
    nonlinear_split:
      num_branches: 3
      method: babsr_like
# bab:
#   timeout: 120  # Timeout threshold for branch and bound. Increase for verifying more points.
#   branching:  # Parameters for branching heuristics.
#     reduceop: min  # Reduction function for the branching heuristic scores, min or max. Using max can be better on some models.
#     method: kfsb  # babsr is fast but less accurate; fsb is slow but most accurate; kfsb is usually a balance.
#     candidates: 3  # Number of candidates to consider in fsb and kfsb. More leads to slower but better branching. 3 is typically good enough.
